{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# install libs\nimport os\n\nif not os.path.exists('/kaggle/working/flag'):\n    !pip3 install huggingsound --user\n    !pip3 install num2words\n\n    with open('flag', 'w') as fout:\n        fout.write('flag')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:26:46.810904Z","iopub.execute_input":"2022-05-08T16:26:46.81124Z","iopub.status.idle":"2022-05-08T16:27:13.586074Z","shell.execute_reply.started":"2022-05-08T16:26:46.811127Z","shell.execute_reply":"2022-05-08T16:27:13.585241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility\nfrom sklearn.model_selection import KFold\nfrom num2words import num2words\nimport numpy as np\nimport pandas as pd \nimport random\nimport json\nimport yaml\n\n# Audio\nimport librosa\nimport IPython.display as ipd\nfrom tqdm.notebook import tqdm\n\n# Graphics \n%matplotlib inline\nimport librosa.display\nimport matplotlib.pyplot as plt\n\n# Models\nimport torch\nfrom huggingsound import SpeechRecognitionModel\nfrom huggingsound import TrainingArguments, ModelArguments, TokenSet\nimport pytorch_lightning as pl\nfrom sklearn.model_selection import train_test_split\n\n# Metrics\nfrom jiwer import wer\n\n# Consts and Settings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42\n\nMODEL_ID = 'jonatasgrosman/wav2vec2-large-xlsr-53-russian'\n\nPATH = '/kaggle/input/speech-to-text-russian/data/data/'\nPATH_TRAIN = PATH + 'train_wavs/'\nPATH_TEST = PATH + 'test_wavs/'\nwork_mode = True\nPATH_WORKING = '/kaggle/working/' if work_mode else '/kaggle/input/speech-to-text-russian/results-4/' \n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)\n\ntorch.cuda.empty_cache()\n\ncuda = torch.cuda.is_available()\ndevice_name = 'cuda' if cuda else 'cpu'\ndevice = torch.device(device_name)\n\ndevice_name","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:27:13.59001Z","iopub.execute_input":"2022-05-08T16:27:13.590246Z","iopub.status.idle":"2022-05-08T16:27:22.432803Z","shell.execute_reply.started":"2022-05-08T16:27:13.590216Z","shell.execute_reply":"2022-05-08T16:27:22.432066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and listen to the audio file\nexample_file = PATH_TRAIN + '0.wav'\naudio, sample_rate = librosa.load(example_file)\n\nipd.Audio(example_file, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:27:22.434238Z","iopub.execute_input":"2022-05-08T16:27:22.434488Z","iopub.status.idle":"2022-05-08T16:27:23.360752Z","shell.execute_reply.started":"2022-05-08T16:27:22.434453Z","shell.execute_reply":"2022-05-08T16:27:23.360054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot our example audio file's waveform\nplt.rcParams['figure.figsize'] = (20,5)\nplt.title('Waveform of Audio Example')\nplt.ylabel('Amplitude')\n\n_ = librosa.display.waveshow(audio)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:27:23.362518Z","iopub.execute_input":"2022-05-08T16:27:23.362752Z","iopub.status.idle":"2022-05-08T16:27:23.895128Z","shell.execute_reply.started":"2022-05-08T16:27:23.362723Z","shell.execute_reply":"2022-05-08T16:27:23.894482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mel_spec = librosa.feature.melspectrogram(audio, sr=sample_rate)\nmel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n\nlibrosa.display.specshow(\n    mel_spec_db, x_axis='time', y_axis='mel')\nplt.colorbar()\nplt.title('Mel Spectrogram')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:27:23.896265Z","iopub.execute_input":"2022-05-08T16:27:23.897348Z","iopub.status.idle":"2022-05-08T16:27:24.312023Z","shell.execute_reply.started":"2022-05-08T16:27:23.897307Z","shell.execute_reply":"2022-05-08T16:27:24.311335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get spectrogram using Librosa's Short-Time Fourier Transform (stft)\nspec = np.abs(librosa.stft(audio))\nspec_db = librosa.amplitude_to_db(spec, ref=np.max)  # Decibels\n\n# Use log scale to view frequencies\nlibrosa.display.specshow(spec_db, y_axis='log', x_axis='time')\nplt.colorbar()\nplt.title('Audio Spectrogram');","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:27:24.31309Z","iopub.execute_input":"2022-05-08T16:27:24.31344Z","iopub.status.idle":"2022-05-08T16:27:24.994767Z","shell.execute_reply.started":"2022-05-08T16:27:24.313406Z","shell.execute_reply":"2022-05-08T16:27:24.994055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter transcription\ntrain_labels = pd.read_csv(PATH + 'train_labels.csv')\n\ndef str_filter(utterance):\n    bad_symbols = set(['0','1','2','3','4','5','6','7','8','9','_','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','і','ї','ґ'])\n    utterance.strip('0123456789.- _')\n    utterance = ' '.join([num2words(i, lang='ru') if i.isdigit() else i for i in utterance.split()])\n    utterance = ''.join([c for c in utterance if c not in bad_symbols])\n    return utterance\n\ntrain_labels['Expected'] = train_labels['Expected'].apply(str_filter)\ntrain_labels.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:27:24.996059Z","iopub.execute_input":"2022-05-08T16:27:24.99688Z","iopub.status.idle":"2022-05-08T16:27:26.72977Z","shell.execute_reply.started":"2022-05-08T16:27:24.996824Z","shell.execute_reply":"2022-05-08T16:27:26.729059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub = pd.read_csv(PATH + 'sample_submission.csv')\nsample_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:27:26.731165Z","iopub.execute_input":"2022-05-08T16:27:26.731451Z","iopub.status.idle":"2022-05-08T16:27:26.750396Z","shell.execute_reply.started":"2022-05-08T16:27:26.731414Z","shell.execute_reply":"2022-05-08T16:27:26.749768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting without Fine-tuning","metadata":{}},{"cell_type":"code","source":"def predict(sample_sub, model=None, batch_size=1, path_test=PATH_TEST):\n    audio_path = [path_test + str(row['Id']) + '.wav' for i, row in sample_sub.iterrows()]\n    \n    if model is None:\n        model = SpeechRecognitionModel(MODEL_ID, device=device_name)\n        \n    transcriptions = model.transcribe(audio_path, batch_size=batch_size)\n    sample_sub['Predicted'] = [transcriptions[i]['transcription'] for i in range(len(transcriptions))]\n    \n    sample_sub = sample_sub.set_index('Id')\n    return sample_sub","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:27:26.751688Z","iopub.execute_input":"2022-05-08T16:27:26.751933Z","iopub.status.idle":"2022-05-08T16:27:26.75898Z","shell.execute_reply.started":"2022-05-08T16:27:26.7519Z","shell.execute_reply":"2022-05-08T16:27:26.758091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict = predict(sample_sub.iloc[:100])\n# predict.to_csv('answer_no_tuning.csv')\n# predict.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:27:26.761857Z","iopub.execute_input":"2022-05-08T16:27:26.762304Z","iopub.status.idle":"2022-05-08T16:27:26.766764Z","shell.execute_reply.started":"2022-05-08T16:27:26.762271Z","shell.execute_reply":"2022-05-08T16:27:26.765976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tuning model","metadata":{}},{"cell_type":"code","source":"model = SpeechRecognitionModel(MODEL_ID, device=device_name)\noutput_dir = './fine-tuned/'\n\n# first of all, you need to define your model's token set\ntokens = ['а','б','в','г','д','е','ж','з','и','й','к','л','м','н','о','п','р','с','т','у','ф','х','ц','ч','ш','щ','ъ','ы','ь','э','ю','я']\ntoken_set = TokenSet(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:27:26.767776Z","iopub.execute_input":"2022-05-08T16:27:26.767988Z","iopub.status.idle":"2022-05-08T16:28:37.89329Z","shell.execute_reply.started":"2022-05-08T16:27:26.767958Z","shell.execute_reply":"2022-05-08T16:28:37.892605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fine_tune(model, X, train_batch_size=3, eval_batch_size=3, test_size=0.2):\n    labels_train, labels_val = train_test_split(X, test_size=test_size, random_state=SEED)\n\n    train_data = [\n        {'path': PATH_TRAIN + str(row['Id']) + '.wav', 'transcription': row['Expected']}\n        for i, row in labels_train.iterrows()\n    ]\n\n    eval_data = [\n        {'path': PATH_TRAIN + str(row['Id']) + '.wav', 'transcription': row['Expected']}\n        for i, row in labels_val.iterrows()\n    ]\n    \n    # and finally, fine-tune your model\n    training_args = TrainingArguments()\n    training_args.per_device_train_batch_size = train_batch_size\n    training_args.per_device_eval_batch_size = eval_batch_size\n    training_args.logging_steps = 1000\n    training_args.overwrite_output_dir = True\n\n\n    model.finetune(\n        output_dir, \n        train_data=train_data, \n        eval_data=eval_data,\n        token_set=token_set,\n        training_args=training_args,\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:29:09.893149Z","iopub.execute_input":"2022-05-08T16:29:09.893666Z","iopub.status.idle":"2022-05-08T16:29:09.901864Z","shell.execute_reply.started":"2022-05-08T16:29:09.893626Z","shell.execute_reply":"2022-05-08T16:29:09.90112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5)\n\nfor _, test_index in kf.split(train_labels):\n    X = train_labels.iloc[test_index]\n    model = fine_tune(model, X)\n    predict = predict(sample_sub, model=model)\n    predict.to_csv('answer_with_tuning.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:29:11.490893Z","iopub.execute_input":"2022-05-08T16:29:11.491437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:29:00.43179Z","iopub.status.idle":"2022-05-08T16:29:00.433821Z","shell.execute_reply.started":"2022-05-08T16:29:00.433585Z","shell.execute_reply":"2022-05-08T16:29:00.433613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}